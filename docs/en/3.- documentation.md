# DEVELOPMENT DOCUMENTATION

This section outlines step-by-step how the Delta Assistance project was developedâ€”an electric wheelchair controlled by eye-tracking, aimed at improving mobility for individuals with paralysis or severe motor disabilities. Below are the detailed phases of the process, from initial concept to final system integration.

## SYSTEM DESIGN

The architecture was based on three main components:
- An eye-tracking interface that captures the user's gaze position to interpret movement commands within a digital interface designed using Qt Designer.
- An Arduino Mega microcontroller to interpret the signals sent from the Raspberry Pi and transmit commands to the wheelchair motors.
- A motor control system that moves the wheelchair in the desired direction safely.

## SOFTWARE DEVELOPMENT

The software was divided into two layers. The first is a visual interface developed in Python, which uses computer vision to analyze eye position and determine whether the user wants to move forward, backward, left, right, or stop. This interface translates the gaze direction into signals sent to the second layer: the microcontroller.  
The Arduino Mega was programmed in C++ using the Arduino environment. Its function is to receive commands, generate the appropriate motor control signals (using PWM), and ensure precise and safe control. Debugging messages were also included to facilitate testing.

## HARDWARE DEVELOPMENT

On the physical side, a standard wheelchair was modified by adding electronically controllable motors. A custom PCB was designed to mount the Arduino Mega and other components, such as voltage regulators and connection ports.  
An autonomous power system with rechargeable batteries was integrated, along with a fixed mount for the eye-tracking camera, positioned in front of the user's face. The entire system was built with a focus on ease of maintenance and durability for continuous use.

## INTEGRATION AND CALIBRATION

Once both the software and hardware stages were completed, all modules were integrated. An initial calibration process was designed for each user, allowing adjustment of eye sensitivity values.  
Various lighting conditions and distances were tested to ensure the system functions reliably under real-world scenarios.
